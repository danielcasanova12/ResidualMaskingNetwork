{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytorchcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install json5 imgaug numpy torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from glob import glob\n",
    "\n",
    "EMOTION_DICT = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"sad\": 4,\n",
    "    \"surprise\": 5,\n",
    "    \"neutral\": 6,\n",
    "}\n",
    "\n",
    "\n",
    "class FER2013Folders(Dataset):\n",
    "    def __init__(self, data_path, stage=\"train\", image_size=48, transform=None):\n",
    "        self.data_path = os.path.join(data_path, stage)\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Coletar todos os caminhos de imagem e seus rótulos\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for emotion, label in EMOTION_DICT.items():\n",
    "            emotion_folder = os.path.join(self.data_path, emotion)\n",
    "            if not os.path.exists(emotion_folder):\n",
    "                continue  # Ignorar classes que não existam na pasta\n",
    "            images = glob(os.path.join(emotion_folder, \"*.png\")) + \\\n",
    "                     glob(os.path.join(emotion_folder, \"*.jpg\"))\n",
    "            self.image_paths.extend(images)\n",
    "            self.labels.extend([label] * len(images))\n",
    "\n",
    "        # Use o transform fornecido, ou o transform padrão\n",
    "        self.transform = transform or transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),  # Ajuste para imagens em escala de cinza\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Carregar a imagem em escala de cinza\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Erro ao carregar a imagem: {image_path}\")\n",
    "\n",
    "        # Redimensionar a imagem para o tamanho esperado\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "\n",
    "        # Converter para formato de tensor\n",
    "        image = np.expand_dims(image, axis=-1)  # Adicionar canal\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def fer2013_from_folders(data_path, stage=\"train\", image_size=48, transform=None):\n",
    "    \"\"\"\n",
    "    Função para inicializar o dataset a partir de pastas.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Caminho para o diretório contendo as imagens.\n",
    "    stage : str\n",
    "        Estágio do dataset ('train', 'val', 'test').\n",
    "    image_size : int\n",
    "        Tamanho da imagem.\n",
    "    transform : torchvision.transforms.Compose\n",
    "        Transformações a serem aplicadas às imagens.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    FER2013Folders\n",
    "        Uma instância do dataset.\n",
    "    \"\"\"\n",
    "    return FER2013Folders(data_path, stage, image_size, transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trainer..\n",
      "{'data_path': 'data/', 'image_size': 224, 'in_channels': 3, 'num_classes': 7, 'arch': 'resnet34', 'lr': 0.0001, 'weighted_loss': 0, 'momentum': 0.9, 'weight_decay': 0.001, 'distributed': 0, 'batch_size': 48, 'num_workers': 6, 'device': 'cuda:0', 'max_epoch_num': 50, 'max_plateau_count': 8, 'plateau_patience': 2, 'steplr': 50, 'log_dir': 'log', 'checkpoint_dir': 'checkpoint', 'model_name': 'test', 'best_model_name': 'best_modelv2.pth', 'last_model_name': 'last_modelv2.pth', 'cwd': 'f:\\\\Git\\\\ResidualMaskingNetwork'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E001  1.305/1.065/1.065 50.054/59.535/59.535 | p00  Time 0:04:25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E002  0.998/0.977/0.977 62.295/63.615/63.615 | p00  Time 0:08:48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E003  0.914/0.936/0.936 65.737/65.512/65.512 | p00  Time 0:13:12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E004  0.846/0.927/0.927 68.383/66.365/66.365 | p00  Time 0:17:37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E005  0.791/0.890/0.890 70.682/68.239/68.239 | p00  Time 0:22:00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E006  0.737/0.906/0.890 72.685/67.154/68.239 | p01  Time 0:26:23\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E007  0.692/0.955/0.890 74.426/66.981/68.239 | p02  Time 0:30:46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E008  0.645/0.900/0.890 76.155/68.209/68.239 | p03  Time 0:35:16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E009  0.483/0.884/0.884 82.806/70.884/70.884 | p00  Time 0:39:40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E010  0.411/0.905/0.884 85.622/70.662/70.884 | p01  Time 0:44:02\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E011  0.374/0.930/0.930 87.032/70.912/70.912 | p00  Time 0:48:25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E012  0.342/0.948/0.930 88.159/70.301/70.912 | p01  Time 0:52:48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E013  0.310/0.990/0.930 89.364/70.429/70.912 | p02  Time 0:57:11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E014  0.282/1.015/0.930 90.197/70.587/70.912 | p03  Time 1:01:34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E015  0.256/1.042/0.930 91.329/70.651/70.912 | p04  Time 1:05:56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E016  0.247/1.024/0.930 91.750/70.476/70.912 | p05  Time 1:10:19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E017  0.248/1.028/0.930 91.642/70.643/70.912 | p06  Time 1:14:41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E018  0.240/1.026/0.930 91.820/70.095/70.912 | p07  Time 1:19:04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E019  0.234/1.033/0.930 92.254/70.782/70.912 | p08  Time 1:23:27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last model saved to f:\\Git\\ResidualMaskingNetwork\\checkpoint\\last_modelv2.pth\n",
      "\n",
      "E020  0.234/1.051/1.051 92.172/70.976/70.976 | p00  Time 1:27:50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E021  0.235/1.062/1.051 92.200/70.329/70.976 | p01  Time 1:32:12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E022  0.231/1.056/1.051 92.252/70.373/70.976 | p02  Time 1:36:35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E023  0.227/1.062/1.051 92.257/70.881/70.976 | p03  Time 1:40:58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E024  0.226/1.059/1.051 92.444/70.568/70.976 | p04  Time 1:45:21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E025  0.218/1.068/1.051 92.732/70.365/70.976 | p05  Time 1:49:44\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E026  0.221/1.057/1.051 92.621/70.623/70.976 | p06  Time 1:54:07\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E027  0.215/1.072/1.051 92.833/70.893/70.976 | p07  Time 1:58:30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E028  0.215/1.063/1.051 92.903/70.559/70.976 | p08  Time 2:02:52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E029  0.212/1.080/1.051 93.111/70.440/70.976 | p09  Time 2:07:14\n",
      "\n",
      "Calc acc on private test with tta..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):      \n",
      "  File \"f:\\Git\\ResidualMaskingNetwork\\trainers\\tta_trainer.py\", line 341, in train\n",
      "    self._test_acc = self._calc_acc_on_private_test_with_tta()\n",
      "  File \"f:\\Git\\ResidualMaskingNetwork\\trainers\\tta_trainer.py\", line 303, in _calc_acc_on_private_test_with_tta\n",
      "    outputs = self._model(images)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"f:\\Git\\ResidualMaskingNetwork\\models\\resnet.py\", line 269, in forward\n",
      "    x = self.conv1(x)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 458, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 454, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 10, 3, 224, 224]\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Git\\ResidualMaskingNetwork\\trainers\\tta_trainer.py\", line 351, in train\n",
      "    state = torch.load(self._checkpoint_path)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 1065, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 468, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py\", line 449, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'f:\\\\Git\\\\ResidualMaskingNetwork\\\\checkpoint\\\\resnet34_test_2024Dec29_01.07'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import imgaug\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "imgaug.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torchvision import transforms\n",
    "\n",
    "import models\n",
    "from models import segmentation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converte a imagem para tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normaliza para [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def main(config_path):\n",
    "    \"\"\"\n",
    "    This is the main function to make the training up\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config_path : srt\n",
    "        path to config file\n",
    "    \"\"\"\n",
    "    # load configs and set random seed\n",
    "    configs = json.load(open(config_path))\n",
    "    configs[\"cwd\"] = os.getcwd()\n",
    "\n",
    "    # load model and data_loader\n",
    "    model = get_model(configs)\n",
    "\n",
    "    train_set, val_set, test_set = get_dataset(configs)\n",
    "\n",
    "    # init trainer and make a training\n",
    "    # from trainers.fer2013_trainer import FER2013Trainer\n",
    "    from trainers.tta_trainer import FER2013Trainer\n",
    "\n",
    "    # from trainers.centerloss_trainer import FER2013Trainer\n",
    "    trainer = FER2013Trainer(model, train_set, val_set, test_set, configs)\n",
    "\n",
    "    if configs[\"distributed\"] == 1:\n",
    "        ngpus = torch.cuda.device_count()\n",
    "        mp.spawn(trainer.train, nprocs=ngpus, args=())\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "def get_model(configs):\n",
    "    \"\"\"\n",
    "    This function get raw models from models package\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    configs : dict\n",
    "        configs dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return models.__dict__[configs[\"arch\"]]\n",
    "    except KeyError:\n",
    "        return segmentation.__dict__[configs[\"arch\"]]\n",
    "\n",
    "def get_model2(configs):\n",
    "    \"\"\"\n",
    "    This function gets the model from the models package.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    configs : dict\n",
    "        Configuration dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the model with configurations\n",
    "        model = models.__dict__[configs[\"arch\"]](num_classes=configs[\"num_classes\"])\n",
    "        return model\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"The architecture '{configs['arch']}' is not defined in the models package.\")\n",
    "\n",
    "\n",
    "def get_dataset(configs):\n",
    "    \"\"\"\n",
    "    This function gets raw datasets for training, validation, and testing.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    configs : dict\n",
    "        Configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        train_set, val_set, test_set datasets.\n",
    "    \"\"\"\n",
    "    from torchvision.transforms import transforms\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "             \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_set = fer2013_from_folders(\"train\", configs)\n",
    "    val_set = fer2013_from_folders(\"val\", configs)\n",
    "    test_set = fer2013_from_folders(\"test\", configs, tta=True, tta_size=10)\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"./configs/fer2013_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from utils.datasets.fer2013dataset import fer2013\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def test_model(model, test_set, configs):\n",
    "#     model.eval()\n",
    "#     device = torch.device(configs[\"device\"])\n",
    "#     model.to(device)\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_set, batch_size=1, shuffle=False, num_workers=configs[\"num_workers\"]\n",
    "#     )\n",
    "\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "#             if len(images.shape) == 5:  # Verifica se o formato é [batch, tta_size, C, H, W]\n",
    "#                 batch_size, tta_size, C, H, W = images.size()\n",
    "#                 images = images.view(-1, C, H, W).to(device)  # Reorganiza como [batch * tta_size, C, H, W]\n",
    "#                 outputs = model(images)\n",
    "#                 outputs = outputs.view(batch_size, tta_size, -1)  # Volta para [batch, tta_size, num_classes]\n",
    "#                 outputs = torch.mean(outputs, dim=1)  # Faz a média ao longo do TTA\n",
    "#             else:\n",
    "#                 images = images.to(device)\n",
    "#                 outputs = model(images)\n",
    "\n",
    "#             labels = labels.to(device)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         cm = confusion_matrix(all_labels, all_preds)\n",
    "#         cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Mantendo os valores entre 0 e 1\n",
    "\n",
    "#         # Relatório de classificação\n",
    "#         print(\"Classification Report:\")\n",
    "#         print(classification_report(all_labels, all_preds))\n",
    "\n",
    "#         # Configurando o gráfico\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         sns.heatmap(cm_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "#                     xticklabels=test_set._emotions.columns, \n",
    "#                     yticklabels=test_set._emotions.columns)\n",
    "\n",
    "#         plt.xlabel(\"Predicted\")\n",
    "#         plt.ylabel(\"True\")\n",
    "#         plt.title(\"Residual Masking Network\")\n",
    "#         plt.show()\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load configs\n",
    "#     configs = json.load(open(\"./configs/fer2013_config.json\"))\n",
    "#     configs[\"cwd\"] = os.getcwd()\n",
    "\n",
    "#     # Load model and dataset\n",
    "#     checkpoint_path = os.path.join(configs[\"checkpoint_dir\"], f\"last_model.pth\")\n",
    "#     print(f\"Loading model from {checkpoint_path}\")\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# # Verifique as chaves do checkpoint\n",
    "#     print(\"Keys in checkpoint:\", checkpoint.keys())\n",
    "\n",
    "#     # Carregar o estado do modelo\n",
    "#     state_dict = checkpoint[\"net\"]  # Use a chave correta para os pesos do modelo\n",
    "#     model = get_model2(configs)\n",
    "#     model.load_state_dict(state_dict)\n",
    "\n",
    "#     # Load private dataset\n",
    "#     _, _, test_set = get_dataset(configs)\n",
    "\n",
    "#     # Test model\n",
    "#     test_model(model, test_set, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo emotenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResEmoteNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResEmoteNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SEBlock(256)\n",
    "        \n",
    "        self.res_block1 = ResidualBlock(256, 512, stride=2)\n",
    "        self.res_block2 = ResidualBlock(512, 1024, stride=2)\n",
    "        self.res_block3 = ResidualBlock(1024, 2048, stride=2)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(256, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.se(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset class for the custom dataset\n",
    "class Four4All(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = self.labels.iloc[idx, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cuda:0\n",
      "Train batch: Image shape torch.Size([16, 3, 64, 64]), Label shape torch.Size([16])\n",
      "Validation batch: Image shape torch.Size([16, 3, 64, 64]), Label shape torch.Size([16])\n",
      "Test batch: Image shape torch.Size([16, 3, 64, 64]), Label shape torch.Size([16])\n",
      "80,238,599 total parameters.\n",
      "Carregando checkpoint anterior...\n",
      "Checkpoint carregado com validação de melhor acurácia: 0.7712\n",
      "Retomando do Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/80: 100%|██████████| 2264/2264 [08:31<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train Loss: 0.4912, Train Accuracy: 0.8313, Test Loss: 0.5770, Test Accuracy: 0.7952, Validation Loss: 0.6280, Validation Accuracy: 0.7843\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 0.4656, Train Accuracy: 0.8382, Test Loss: 0.5449, Test Accuracy: 0.8113, Validation Loss: 0.5739, Validation Accuracy: 0.8149\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train Loss: 0.4451, Train Accuracy: 0.8462, Test Loss: 0.5272, Test Accuracy: 0.8194, Validation Loss: 0.5759, Validation Accuracy: 0.8060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train Loss: 0.4209, Train Accuracy: 0.8567, Test Loss: 0.5536, Test Accuracy: 0.8119, Validation Loss: 0.5670, Validation Accuracy: 0.8041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Train Loss: 0.3950, Train Accuracy: 0.8658, Test Loss: 0.4982, Test Accuracy: 0.8339, Validation Loss: 0.5389, Validation Accuracy: 0.8096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 0.3644, Train Accuracy: 0.8775, Test Loss: 0.4885, Test Accuracy: 0.8397, Validation Loss: 0.5434, Validation Accuracy: 0.8219\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Train Loss: 0.3455, Train Accuracy: 0.8811, Test Loss: 0.5252, Test Accuracy: 0.8258, Validation Loss: 0.5273, Validation Accuracy: 0.8233\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Train Loss: 0.3214, Train Accuracy: 0.8899, Test Loss: 0.5274, Test Accuracy: 0.8169, Validation Loss: 0.5791, Validation Accuracy: 0.8127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Train Loss: 0.3113, Train Accuracy: 0.8943, Test Loss: 0.4589, Test Accuracy: 0.8464, Validation Loss: 0.4894, Validation Accuracy: 0.8442\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Train Loss: 0.2922, Train Accuracy: 0.9005, Test Loss: 0.4449, Test Accuracy: 0.8559, Validation Loss: 0.4819, Validation Accuracy: 0.8470\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Train Loss: 0.2727, Train Accuracy: 0.9082, Test Loss: 0.4466, Test Accuracy: 0.8478, Validation Loss: 0.5111, Validation Accuracy: 0.8370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Train Loss: 0.2600, Train Accuracy: 0.9130, Test Loss: 0.5333, Test Accuracy: 0.8283, Validation Loss: 0.5396, Validation Accuracy: 0.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/80: 100%|██████████| 2264/2264 [04:45<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Train Loss: 0.2522, Train Accuracy: 0.9159, Test Loss: 0.4517, Test Accuracy: 0.8545, Validation Loss: 0.4590, Validation Accuracy: 0.8554\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Train Loss: 0.2291, Train Accuracy: 0.9227, Test Loss: 0.4656, Test Accuracy: 0.8475, Validation Loss: 0.5427, Validation Accuracy: 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Train Loss: 0.2212, Train Accuracy: 0.9277, Test Loss: 0.4259, Test Accuracy: 0.8654, Validation Loss: 0.4766, Validation Accuracy: 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Train Loss: 0.2021, Train Accuracy: 0.9331, Test Loss: 0.4580, Test Accuracy: 0.8562, Validation Loss: 0.4705, Validation Accuracy: 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Train Loss: 0.1999, Train Accuracy: 0.9339, Test Loss: 0.4215, Test Accuracy: 0.8665, Validation Loss: 0.4540, Validation Accuracy: 0.8587\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Train Loss: 0.1839, Train Accuracy: 0.9380, Test Loss: 0.4212, Test Accuracy: 0.8673, Validation Loss: 0.4602, Validation Accuracy: 0.8659\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Train Loss: 0.1783, Train Accuracy: 0.9416, Test Loss: 0.4193, Test Accuracy: 0.8676, Validation Loss: 0.4411, Validation Accuracy: 0.8687\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/80: 100%|██████████| 2264/2264 [04:43<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Train Loss: 0.1625, Train Accuracy: 0.9462, Test Loss: 0.4888, Test Accuracy: 0.8531, Validation Loss: 0.5034, Validation Accuracy: 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Train Loss: 0.1619, Train Accuracy: 0.9465, Test Loss: 0.4198, Test Accuracy: 0.8779, Validation Loss: 0.4541, Validation Accuracy: 0.8685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Train Loss: 0.1533, Train Accuracy: 0.9490, Test Loss: 0.4013, Test Accuracy: 0.8785, Validation Loss: 0.4400, Validation Accuracy: 0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Train Loss: 0.1480, Train Accuracy: 0.9515, Test Loss: 0.3702, Test Accuracy: 0.8807, Validation Loss: 0.4395, Validation Accuracy: 0.8696\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Train Loss: 0.1453, Train Accuracy: 0.9520, Test Loss: 0.4144, Test Accuracy: 0.8793, Validation Loss: 0.4754, Validation Accuracy: 0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/80: 100%|██████████| 2264/2264 [04:43<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Train Loss: 0.1284, Train Accuracy: 0.9583, Test Loss: 0.4256, Test Accuracy: 0.8793, Validation Loss: 0.4390, Validation Accuracy: 0.8729\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Train Loss: 0.1271, Train Accuracy: 0.9588, Test Loss: 0.4716, Test Accuracy: 0.8735, Validation Loss: 0.4716, Validation Accuracy: 0.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 0.1214, Train Accuracy: 0.9605, Test Loss: 0.3927, Test Accuracy: 0.8877, Validation Loss: 0.4283, Validation Accuracy: 0.8818\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Train Loss: 0.1179, Train Accuracy: 0.9621, Test Loss: 0.4320, Test Accuracy: 0.8746, Validation Loss: 0.4456, Validation Accuracy: 0.8721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Train Loss: 0.1192, Train Accuracy: 0.9598, Test Loss: 0.3802, Test Accuracy: 0.8835, Validation Loss: 0.4115, Validation Accuracy: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Train Loss: 0.1078, Train Accuracy: 0.9647, Test Loss: 0.3970, Test Accuracy: 0.8849, Validation Loss: 0.4324, Validation Accuracy: 0.8765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Train Loss: 0.1032, Train Accuracy: 0.9656, Test Loss: 0.4358, Test Accuracy: 0.8704, Validation Loss: 0.4486, Validation Accuracy: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Train Loss: 0.1091, Train Accuracy: 0.9636, Test Loss: 0.3915, Test Accuracy: 0.8857, Validation Loss: 0.4362, Validation Accuracy: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Train Loss: 0.0947, Train Accuracy: 0.9696, Test Loss: 0.3847, Test Accuracy: 0.8902, Validation Loss: 0.4179, Validation Accuracy: 0.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Train Loss: 0.0942, Train Accuracy: 0.9698, Test Loss: 0.4036, Test Accuracy: 0.8882, Validation Loss: 0.4653, Validation Accuracy: 0.8732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Train Loss: 0.0893, Train Accuracy: 0.9710, Test Loss: 0.4090, Test Accuracy: 0.8910, Validation Loss: 0.4160, Validation Accuracy: 0.8913\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Train Loss: 0.0929, Train Accuracy: 0.9691, Test Loss: 0.3657, Test Accuracy: 0.8952, Validation Loss: 0.3806, Validation Accuracy: 0.8910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/80: 100%|██████████| 2264/2264 [04:43<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Train Loss: 0.0859, Train Accuracy: 0.9711, Test Loss: 0.4575, Test Accuracy: 0.8827, Validation Loss: 0.4397, Validation Accuracy: 0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Train Loss: 0.0855, Train Accuracy: 0.9710, Test Loss: 0.4380, Test Accuracy: 0.8832, Validation Loss: 0.4715, Validation Accuracy: 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Train Loss: 0.0803, Train Accuracy: 0.9729, Test Loss: 0.4117, Test Accuracy: 0.8841, Validation Loss: 0.4161, Validation Accuracy: 0.8868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Train Loss: 0.0835, Train Accuracy: 0.9724, Test Loss: 0.4242, Test Accuracy: 0.8877, Validation Loss: 0.4203, Validation Accuracy: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Train Loss: 0.0754, Train Accuracy: 0.9757, Test Loss: 0.3869, Test Accuracy: 0.9013, Validation Loss: 0.3975, Validation Accuracy: 0.8986\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.8986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Train Loss: 0.0761, Train Accuracy: 0.9747, Test Loss: 0.5303, Test Accuracy: 0.8673, Validation Loss: 0.4819, Validation Accuracy: 0.8774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Train Loss: 0.0714, Train Accuracy: 0.9772, Test Loss: 0.4209, Test Accuracy: 0.8902, Validation Loss: 0.4366, Validation Accuracy: 0.8910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Train Loss: 0.0773, Train Accuracy: 0.9750, Test Loss: 0.4167, Test Accuracy: 0.8885, Validation Loss: 0.4250, Validation Accuracy: 0.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Train Loss: 0.0676, Train Accuracy: 0.9770, Test Loss: 0.4292, Test Accuracy: 0.8829, Validation Loss: 0.4592, Validation Accuracy: 0.8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Train Loss: 0.0693, Train Accuracy: 0.9777, Test Loss: 0.4247, Test Accuracy: 0.8843, Validation Loss: 0.4528, Validation Accuracy: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Train Loss: 0.0707, Train Accuracy: 0.9759, Test Loss: 0.3792, Test Accuracy: 0.8944, Validation Loss: 0.4025, Validation Accuracy: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Train Loss: 0.0663, Train Accuracy: 0.9780, Test Loss: 0.3919, Test Accuracy: 0.8963, Validation Loss: 0.4295, Validation Accuracy: 0.9008\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Train Loss: 0.0635, Train Accuracy: 0.9795, Test Loss: 0.4368, Test Accuracy: 0.8829, Validation Loss: 0.3890, Validation Accuracy: 0.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/80: 100%|██████████| 2264/2264 [04:43<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Train Loss: 0.0616, Train Accuracy: 0.9799, Test Loss: 0.3908, Test Accuracy: 0.8986, Validation Loss: 0.4151, Validation Accuracy: 0.8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Train Loss: 0.0551, Train Accuracy: 0.9814, Test Loss: 0.4261, Test Accuracy: 0.8941, Validation Loss: 0.4338, Validation Accuracy: 0.9013\n",
      "Melhor modelo salvo com nova acurácia de validação: 0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Train Loss: 0.0609, Train Accuracy: 0.9796, Test Loss: 0.3930, Test Accuracy: 0.8988, Validation Loss: 0.4478, Validation Accuracy: 0.8902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Train Loss: 0.0605, Train Accuracy: 0.9795, Test Loss: 0.3751, Test Accuracy: 0.8944, Validation Loss: 0.4113, Validation Accuracy: 0.8885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/80: 100%|██████████| 2264/2264 [04:42<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Train Loss: 0.0613, Train Accuracy: 0.9798, Test Loss: 0.4358, Test Accuracy: 0.8899, Validation Loss: 0.4497, Validation Accuracy: 0.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/80: 100%|██████████| 2264/2264 [08:04<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Train Loss: 0.0596, Train Accuracy: 0.9801, Test Loss: 0.4070, Test Accuracy: 0.9025, Validation Loss: 0.4554, Validation Accuracy: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/80: 100%|██████████| 2264/2264 [04:54<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Train Loss: 0.0552, Train Accuracy: 0.9815, Test Loss: 0.4317, Test Accuracy: 0.8974, Validation Loss: 0.4716, Validation Accuracy: 0.8983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/80: 100%|██████████| 2264/2264 [04:41<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Train Loss: 0.0621, Train Accuracy: 0.9803, Test Loss: 0.4391, Test Accuracy: 0.8815, Validation Loss: 0.4201, Validation Accuracy: 0.8916\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "# Transform the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = Four4All(csv_file='data2/fer/train_labels.csv',\n",
    "                         img_dir='data2/fer/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "train_image, train_label = next(iter(train_loader))\n",
    "\n",
    "\n",
    "val_dataset = Four4All(csv_file='data2/fer/val_labels.csv', \n",
    "                       img_dir='data2/fer/val/', transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "val_image, val_label = next(iter(val_loader))\n",
    "\n",
    "\n",
    "test_dataset = Four4All(csv_file='data2/fer/test_labels.csv', \n",
    "                        img_dir='data2/fer/test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "test_image, test_label = next(iter(test_loader))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train batch: Image shape {train_image.shape}, Label shape {train_label.shape}\")\n",
    "print(f\"Validation batch: Image shape {val_image.shape}, Label shape {val_label.shape}\")\n",
    "print(f\"Test batch: Image shape {test_image.shape}, Label shape {test_label.shape}\")\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = ResEmoteNet().to(device)\n",
    "\n",
    "\n",
    "# Print the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "patience = 15\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "epoch_counter = 0\n",
    "\n",
    "num_epochs = 80\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Verificar se existe checkpoint anterior e carregar o modelo\n",
    "last_checkpoint_path = './checkpoint/last_emotenet.pth'\n",
    "best_checkpoint_path = './checkpoint/best_emotenet.pth'\n",
    "best_val_acc = 0\n",
    "start_epoch = 0\n",
    "if os.path.exists(last_checkpoint_path):\n",
    "    print(\"Carregando checkpoint anterior...\")\n",
    "    checkpoint = torch.load(last_checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    start_epoch = checkpoint['epoch']  # Recupera a época salva\n",
    "    print(f\"Checkpoint carregado com validação de melhor acurácia: {best_val_acc:.4f}\")\n",
    "    print(f\"Retomando do Epoch {start_epoch}\")\n",
    "\n",
    "# Iniciar o treino\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, \"\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Salvar o checkpoint da última época\n",
    "    torch.save({\n",
    "    'epoch': epoch + 1,  # Salvar a próxima época\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_acc': best_val_acc\n",
    "    }, last_checkpoint_path)\n",
    "\n",
    "\n",
    "    # Salvar o melhor modelo\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_checkpoint_path)\n",
    "        print(\"Melhor modelo salvo com nova acurácia de validação: {:.4f}\".format(best_val_acc))\n",
    "\n",
    "    epoch_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
    "        break\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, epoch_counter+1),\n",
    "    'Train Loss': train_losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Validation Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Validation Accuracy': val_accuracies\n",
    "})\n",
    "df.to_csv('result_four4all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My cnn normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint encontrado. Carregando...\n",
      "Retomando do epoch 1, Melhor Acurácia: 0.0000\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3940, Accuracy: 46.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1651, Accuracy: 56.42%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3330, Accuracy: 49.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1153, Accuracy: 59.07%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2956, Accuracy: 50.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0695, Accuracy: 59.77%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2644, Accuracy: 51.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0193, Accuracy: 61.30%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2367, Accuracy: 53.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0090, Accuracy: 61.99%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2238, Accuracy: 54.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9898, Accuracy: 62.72%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2041, Accuracy: 54.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0119, Accuracy: 61.99%\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1844, Accuracy: 55.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9690, Accuracy: 63.39%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1687, Accuracy: 55.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9499, Accuracy: 64.50%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1548, Accuracy: 56.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9550, Accuracy: 64.47%\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1390, Accuracy: 57.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9287, Accuracy: 65.56%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1316, Accuracy: 57.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8886, Accuracy: 66.54%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1139, Accuracy: 57.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9188, Accuracy: 65.51%\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1014, Accuracy: 58.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8955, Accuracy: 66.98%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0943, Accuracy: 58.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8757, Accuracy: 68.04%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0754, Accuracy: 59.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8946, Accuracy: 67.32%\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0681, Accuracy: 59.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8434, Accuracy: 69.18%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0610, Accuracy: 60.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8807, Accuracy: 68.21%\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0519, Accuracy: 60.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8334, Accuracy: 68.77%\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0513, Accuracy: 60.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8626, Accuracy: 67.79%\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0313, Accuracy: 61.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8060, Accuracy: 70.33%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0243, Accuracy: 61.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8002, Accuracy: 70.19%\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0193, Accuracy: 61.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8218, Accuracy: 70.35%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0084, Accuracy: 62.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8056, Accuracy: 70.88%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0046, Accuracy: 62.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7751, Accuracy: 71.30%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9932, Accuracy: 63.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7611, Accuracy: 71.50%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9962, Accuracy: 62.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7855, Accuracy: 71.19%\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9831, Accuracy: 63.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7729, Accuracy: 72.08%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9760, Accuracy: 63.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7712, Accuracy: 71.64%\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9716, Accuracy: 63.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7444, Accuracy: 72.67%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9678, Accuracy: 63.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8311, Accuracy: 70.47%\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9566, Accuracy: 64.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7781, Accuracy: 71.58%\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9490, Accuracy: 64.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7427, Accuracy: 72.30%\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9407, Accuracy: 64.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7314, Accuracy: 73.36%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9343, Accuracy: 65.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7119, Accuracy: 74.03%\n",
      "Melhor modelo salvo.\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9209, Accuracy: 65.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7341, Accuracy: 73.95%\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9284, Accuracy: 65.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7235, Accuracy: 73.81%\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 189\u001b[0m\n\u001b[0;32m    187\u001b[0m modelo \u001b[38;5;241m=\u001b[39m CustomResNet50(num_classes)\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    188\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(modelo, data_loader_treino, data_loader_validacao, device, specific_hyperparams)\n\u001b[1;32m--> 189\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 119\u001b[0m, in \u001b[0;36mTrainer.train_and_evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    118\u001b[0m train_loss, train_correct, train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader_treino, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinando\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    120\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuração dos hiperparâmetros\n",
    "specific_hyperparams = {\n",
    "    'weight_decay': 0.0001,\n",
    "    'patience': 5,\n",
    "    'momentum': 0.9,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 50,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "# Dataset e DataLoaders\n",
    "class DataLoaderSetup:\n",
    "    def __init__(self, dataset_path, image_size=224, batch_size=64):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_data(self):\n",
    "        transform = {\n",
    "            'treino': transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomResizedCrop(self.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'validacao': transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.CenterCrop(self.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            'treino': datasets.ImageFolder(root=os.path.join(self.dataset_path, 'treino'), transform=transform['treino']),\n",
    "            'validacao': datasets.ImageFolder(root=os.path.join(self.dataset_path, 'validacao'), transform=transform['validacao']),\n",
    "        }\n",
    "\n",
    "        data_loader_treino = DataLoader(data['treino'], batch_size=self.batch_size, shuffle=True)\n",
    "        data_loader_validacao = DataLoader(data['validacao'], batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return data_loader_treino, data_loader_validacao, len(data['treino']), len(data['validacao']), len(data['treino'].classes)\n",
    "\n",
    "# Modelo Customizado\n",
    "class CustomResNet50:\n",
    "    def __init__(self, num_classes):\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False  # Congelar todas as camadas\n",
    "\n",
    "        for param in self.model.layer4.parameters():  # Descongelar o bloco \"layer4\"\n",
    "            param.requires_grad = True\n",
    "\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "# Treinamento\n",
    "class Trainer:\n",
    "    def __init__(self, model, data_loader_treino, data_loader_validacao, device, hyperparams):\n",
    "        self.model = model\n",
    "        self.data_loader_treino = data_loader_treino\n",
    "        self.data_loader_validacao = data_loader_validacao\n",
    "        self.device = device\n",
    "        self.hyperparams = hyperparams\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=hyperparams['lr'], momentum=hyperparams['momentum'], weight_decay=hyperparams['weight_decay'])\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        self.patience = hyperparams['patience']\n",
    "        self.best_accuracy = 0.0\n",
    "\n",
    "        # Diretório para salvar checkpoints\n",
    "        self.checkpoint_dir = './checkpoint'\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # Caminho do último checkpoint\n",
    "        self.last_checkpoint_path = os.path.join(self.checkpoint_dir, \"my_resnet_last.pth\")\n",
    "\n",
    "        # Carregar o checkpoint se existir\n",
    "        self.start_epoch = 0\n",
    "        if os.path.exists(self.last_checkpoint_path):\n",
    "            print(\"Checkpoint encontrado. Carregando...\")\n",
    "            checkpoint = torch.load(self.last_checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            self.start_epoch = checkpoint['epoch'] + 1\n",
    "            self.best_accuracy = checkpoint['best_accuracy']\n",
    "            print(f\"Retomando do epoch {self.start_epoch}, Melhor Acurácia: {self.best_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(\"Nenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.hyperparams['epochs']):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.hyperparams['epochs']}\")\n",
    "\n",
    "            # Treinamento\n",
    "            self.model.train()\n",
    "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "            for inputs, labels in tqdm(self.data_loader_treino, desc=\"Treinando\", leave=False):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                train_correct += (preds == labels).sum().item()\n",
    "                train_total += labels.size(0)\n",
    "\n",
    "            train_accuracy = train_correct / train_total\n",
    "            print(f\"Train Loss: {train_loss / train_total:.4f}, Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "            # Validação\n",
    "            self.model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(self.data_loader_validacao, desc=\"Validando\", leave=False):\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "\n",
    "            val_accuracy = val_correct / val_total\n",
    "            print(f\"Validation Loss: {val_loss / val_total:.4f}, Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Salvar checkpoint da última época\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'best_accuracy': self.best_accuracy\n",
    "            }, self.last_checkpoint_path)\n",
    "\n",
    "            # Salvar o melhor modelo\n",
    "            if val_accuracy > self.best_accuracy:\n",
    "                self.best_accuracy = val_accuracy\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.checkpoint_dir, \"my_best_resnet.pth\"))\n",
    "                print(\"Melhor modelo salvo.\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= self.patience:\n",
    "                print(\"Parando o treinamento devido ao early stopping.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Melhor Acurácia de Validação: {self.best_accuracy:.4f}\")\n",
    "\n",
    "# Configuração do dataset\n",
    "dataset_path = './data2/fer/fer2/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_loader_setup = DataLoaderSetup(dataset_path, image_size=224, batch_size=specific_hyperparams['batch_size'])\n",
    "data_loader_treino, data_loader_validacao, _, _, num_classes = data_loader_setup.load_data()\n",
    "\n",
    "# Inicializar e treinar o modelo\n",
    "modelo = CustomResNet50(num_classes).get_model().to(device)\n",
    "trainer = Trainer(modelo, data_loader_treino, data_loader_validacao, device, specific_hyperparams)\n",
    "trainer.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
